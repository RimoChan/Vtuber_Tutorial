Halo,从6开始提供的代码是用的较简单版本的莉沫酱包含半身），但对于我所在的项目来说复杂度太高。  
所以在这里我也提供了对 **过于简单版本的莉沫酱(仅捕捉头部)** 部分的代码。

---

对了，这里的代码由伊蕾娜提供=-=。如果被merge的话，那么之后出现.5,比如5.5,6.5,7.5(我不太确定能不能写那么后面),应该都是针对过于简单版本的，即只具有脸，五官，后发，前发等common部分的捕捉，我试图把它变得泛化。就是只要输入图层是这么分的，我都试图模拟可以被捕捉的Live2d。

并且后期我的方向会转向png->psd,即输入一张构图不算复杂的二次元头像（正脸），然后直接拆分图层，虽然我现在还在想该如何干净拆分。分类的方式我倒是想到一些。大不了让用户手动=-=。

---



对于原来部分的代码风格暂时没有改变，而且这么写有一种背德感=-=，紧张刺激。  
但我改变了相对路径的引用方式，举例：
原本的`../res/std_face.jpg`改成了`./res/std_face.jpg`。
原本的`深度.yaml`改成了`./4.5/深度.yaml`。

这么改让我能够直接在项目根目录下面运行它们，而不用切换工作目录到子文件夹。也是vscode不太方便。
相对于4你需要增加环境mediapipe。

```cmd
pip install mediapipe==0.10.11
```

**4.5中相对4的改变：**
* 虚境只是封装成类，可以参考[一时休战](5.md)。
* 现实的人脸特征点检测用google的mediapipe替代dlib。
* 增加mediapipe体验.py，你可以在这里体验并且自定义构造点。

dlib捕捉起来断断续续的，特别是超过半侧脸关键点消失。因为查了一下dlib用的是机器学习方法，泛化性差在我这张脸上体现的淋漓尽致。
相对而言mediapipe速度相当快。画面比较丝滑。另外全脸的face_mesh也让我想到制作脸部变形的可能。小幅度的。

![](https://storage.googleapis.com/mediapipe-assets/documentation/mediapipe_face_landmark_fullsize.png)

[这个是面部index索引图原图网址，如果你看不清或者没有加载出来可以到这里。](https://storage.googleapis.com/mediapipe-assets/documentation/mediapipe_face_landmark_fullsize.png)

